"""
Analyse exploratoire et preprocessing du dataset bancaire
Pr√©diction des moments de vie
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
import warnings
warnings.filterwarnings('ignore')

# Configuration graphiques
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

def load_data():
    """Charge les donn√©es"""
    print("üìÇ Chargement des donn√©es...")
    clients_df = pd.read_csv('../data/clients_data.csv')
    life_events_df = pd.read_csv('../data/life_events.csv')
    print(f"   ‚úì {len(clients_df)} clients charg√©s")
    print(f"   ‚úì {len(life_events_df)} √©v√©nements charg√©s")
    return clients_df, life_events_df

def exploratory_analysis(clients_df, life_events_df):
    """Analyse exploratoire des donn√©es"""
    print("\nüìä ANALYSE EXPLORATOIRE")
    print("=" * 70)
    
    # 1. Vue d'ensemble
    print("\n1. Vue d'ensemble des donn√©es clients:")
    print(clients_df.describe())
    
    print("\n2. Distribution des CSP:")
    print(clients_df['csp'].value_counts())
    
    print("\n3. Distribution des situations familiales:")
    print(clients_df['situation_familiale'].value_counts())
    
    # 2. Analyse des moments de vie
    print("\n4. Distribution des moments de vie:")
    event_counts = life_events_df['moment_de_vie'].value_counts()
    print(event_counts)
    
    # 3. Corr√©lations
    print("\n5. Analyse des corr√©lations (features num√©riques):")
    numeric_cols = clients_df.select_dtypes(include=[np.number]).columns
    correlation_matrix = clients_df[numeric_cols].corr()
    print("\nTop 10 corr√©lations les plus fortes:")
    # Extraire les corr√©lations sans la diagonale
    corr_pairs = []
    for i in range(len(correlation_matrix.columns)):
        for j in range(i+1, len(correlation_matrix.columns)):
            corr_pairs.append({
                'feature1': correlation_matrix.columns[i],
                'feature2': correlation_matrix.columns[j],
                'correlation': abs(correlation_matrix.iloc[i, j])
            })
    corr_df = pd.DataFrame(corr_pairs).sort_values('correlation', ascending=False)
    print(corr_df.head(10))
    
    return correlation_matrix

def create_target_variable(clients_df, life_events_df, target_event='achat_immobilier'):
    """Cr√©e la variable cible pour un moment de vie sp√©cifique"""
    print(f"\nüéØ Cr√©ation de la variable cible pour: {target_event}")
    
    # Identifier les clients ayant v√©cu cet √©v√©nement
    target_clients = life_events_df[
        life_events_df['moment_de_vie'] == target_event
    ]['client_id'].unique()
    
    # Cr√©er la variable cible
    clients_df['target'] = clients_df['client_id'].isin(target_clients).astype(int)
    
    # Distribution de la cible
    target_dist = clients_df['target'].value_counts()
    print(f"   Distribution de la cible:")
    print(f"   - Classe 0 (pas d'√©v√©nement): {target_dist[0]} ({target_dist[0]/len(clients_df)*100:.1f}%)")
    print(f"   - Classe 1 (√©v√©nement): {target_dist[1]} ({target_dist[1]/len(clients_df)*100:.1f}%)")
    
    return clients_df

def preprocess_data(clients_df):
    """Preprocessing des donn√©es"""
    print("\nüîß Preprocessing des donn√©es...")
    
    # 1. S√©parer features et target
    if 'target' not in clients_df.columns:
        raise ValueError("La variable target doit √™tre cr√©√©e avant le preprocessing")
    
    target = clients_df['target']
    features_df = clients_df.drop(['client_id', 'target'], axis=1)
    
    # 2. Encoder les variables cat√©gorielles
    categorical_cols = features_df.select_dtypes(include=['object']).columns
    print(f"   Encodage de {len(categorical_cols)} variables cat√©gorielles...")
    
    le_dict = {}
    for col in categorical_cols:
        le = LabelEncoder()
        features_df[col] = le.fit_transform(features_df[col])
        le_dict[col] = le
    
    # 3. Normalisation des features num√©riques
    print("   Normalisation des features num√©riques...")
    scaler = StandardScaler()
    features_scaled = scaler.fit_transform(features_df)
    features_df_scaled = pd.DataFrame(features_scaled, columns=features_df.columns)
    
    print(f"   ‚úì Dataset preprocess√©: {features_df_scaled.shape}")
    
    return features_df_scaled, target, le_dict, scaler

def build_baseline_model(X, y):
    """Construit un mod√®le de base pour √©valuer la pr√©dictibilit√©"""
    print("\nü§ñ Construction d'un mod√®le baseline (Random Forest)...")
    
    # Split train/test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    
    print(f"   Train set: {X_train.shape[0]} samples")
    print(f"   Test set: {X_test.shape[0]} samples")
    
    # Entra√Ænement
    print("   Entra√Ænement du mod√®le...")
    rf_model = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        random_state=42,
        class_weight='balanced'  # Pour g√©rer le d√©s√©quilibre
    )
    rf_model.fit(X_train, y_train)
    
    # Pr√©dictions
    y_pred = rf_model.predict(X_test)
    
    # √âvaluation
    print("\n   üìà Performance du mod√®le:")
    print(classification_report(y_test, y_pred, target_names=['Pas d\'√©v√©nement', '√âv√©nement']))
    
    # Feature importance
    print("\n   üîç Top 10 features les plus importantes:")
    feature_importance = pd.DataFrame({
        'feature': X.columns,
        'importance': rf_model.feature_importances_
    }).sort_values('importance', ascending=False)
    print(feature_importance.head(10).to_string(index=False))
    
    return rf_model, feature_importance

def generate_visualizations(clients_df, life_events_df, correlation_matrix, feature_importance):
    """G√©n√®re les visualisations"""
    print("\nüìä G√©n√©ration des visualisations...")
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('Analyse Exploratoire - Pr√©diction des Moments de Vie', fontsize=16, fontweight='bold')
    
    # 1. Distribution des √¢ges
    axes[0, 0].hist(clients_df['age'], bins=30, edgecolor='black', alpha=0.7)
    axes[0, 0].set_title('Distribution des √¢ges')
    axes[0, 0].set_xlabel('√Çge')
    axes[0, 0].set_ylabel('Fr√©quence')
    axes[0, 0].grid(True, alpha=0.3)
    
    # 2. Distribution des CSP
    csp_counts = clients_df['csp'].value_counts()
    axes[0, 1].bar(range(len(csp_counts)), csp_counts.values)
    axes[0, 1].set_title('Distribution des CSP')
    axes[0, 1].set_xticks(range(len(csp_counts)))
    axes[0, 1].set_xticklabels(csp_counts.index, rotation=45, ha='right')
    axes[0, 1].set_ylabel('Nombre de clients')
    axes[0, 1].grid(True, alpha=0.3)
    
    # 3. Distribution des moments de vie
    event_counts = life_events_df['moment_de_vie'].value_counts()
    axes[0, 2].barh(range(len(event_counts)), event_counts.values)
    axes[0, 2].set_title('Distribution des moments de vie')
    axes[0, 2].set_yticks(range(len(event_counts)))
    axes[0, 2].set_yticklabels(event_counts.index)
    axes[0, 2].set_xlabel('Nombre d\'√©v√©nements')
    axes[0, 2].grid(True, alpha=0.3)
    
    # 4. Revenus vs √âpargne
    axes[1, 0].scatter(clients_df['revenu_mensuel'], clients_df['epargne_totale'], 
                      alpha=0.5, s=10)
    axes[1, 0].set_title('Revenus vs √âpargne')
    axes[1, 0].set_xlabel('Revenu mensuel (‚Ç¨)')
    axes[1, 0].set_ylabel('√âpargne totale (‚Ç¨)')
    axes[1, 0].grid(True, alpha=0.3)
    
    # 5. Heatmap des corr√©lations (top features)
    top_features = feature_importance.head(10)['feature'].tolist()
    if all(feat in correlation_matrix.columns for feat in top_features):
        corr_subset = correlation_matrix.loc[top_features, top_features]
        sns.heatmap(corr_subset, annot=True, fmt='.2f', cmap='coolwarm', 
                   center=0, ax=axes[1, 1], cbar_kws={'label': 'Corr√©lation'})
        axes[1, 1].set_title('Corr√©lations (Top 10 features)')
    else:
        axes[1, 1].text(0.5, 0.5, 'Corr√©lations non disponibles', 
                       ha='center', va='center', transform=axes[1, 1].transAxes)
        axes[1, 1].set_title('Corr√©lations')
    
    # 6. Feature importance
    top_10_features = feature_importance.head(10)
    axes[1, 2].barh(range(len(top_10_features)), top_10_features['importance'])
    axes[1, 2].set_title('Top 10 Features Importantes')
    axes[1, 2].set_yticks(range(len(top_10_features)))
    axes[1, 2].set_yticklabels(top_10_features['feature'])
    axes[1, 2].set_xlabel('Importance')
    axes[1, 2].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('../data/analyse_exploratoire.png', dpi=300, bbox_inches='tight')
    print("   ‚úì Visualisations sauvegard√©es: analyse_exploratoire.png")
    
    plt.close()

def main():
    """Fonction principale"""
    print("üè¶ ANALYSE EXPLORATOIRE - PR√âDICTION DES MOMENTS DE VIE")
    print("=" * 70)
    
    # 1. Charger les donn√©es
    clients_df, life_events_df = load_data()
    
    # 2. Analyse exploratoire
    correlation_matrix = exploratory_analysis(clients_df, life_events_df)
    
    # 3. Cr√©er la variable cible (exemple: achat immobilier)
    clients_df = create_target_variable(clients_df, life_events_df, target_event='achat_immobilier')
    
    # 4. Preprocessing
    X, y, le_dict, scaler = preprocess_data(clients_df)
    
    # 5. Mod√®le baseline
    model, feature_importance = build_baseline_model(X, y)
    
    # 6. Visualisations
    generate_visualizations(clients_df, life_events_df, correlation_matrix, feature_importance)
    
    # 7. Sauvegarder les r√©sultats
    print("\nüíæ Sauvegarde des r√©sultats...")
    feature_importance.to_csv('../data/feature_importance.csv', index=False)
    print("   ‚úì feature_importance.csv")
    
    print("\n‚úÖ Analyse termin√©e avec succ√®s!")
    print("\nüìã Prochaines √©tapes recommand√©es:")
    print("   1. Tester diff√©rents mod√®les (XGBoost, LightGBM, Neural Networks)")
    print("   2. Optimiser les hyperparam√®tres")
    print("   3. Cr√©er des features d'interaction")
    print("   4. Impl√©menter une validation crois√©e stratifi√©e")
    print("   5. Analyser les erreurs de pr√©diction")

if __name__ == "__main__":
    main()
